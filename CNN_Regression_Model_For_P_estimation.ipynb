{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a2ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fnmatch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "import time\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dae601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lab_tested_data = pd.read_csv('C:\\\\Users\\\\Mahmood Yousaf\\\\Desktop\\\\FYP\\\\Final-Year-Project-Soil-Analysis-using-machine-learning\\\\Soil_Lab_Results - Sheet1.csv')\n",
    "Lab_tested_data = Lab_tested_data.fillna(method='ffill')\n",
    "Lab_tested_data.to_csv(\"Pre_processed_lab_tested_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb892ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_ID = Lab_tested_data.iloc[:,0]\n",
    "P_Value = Lab_tested_data.iloc[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afaa19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    Labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        x = filename.split(\"_\")\n",
    "        id = float(x[0])\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        img = img[800:1100,800:1100]\n",
    "        kernel = np.array([[-1,-1,-1], \n",
    "                   [-1, 9,-1],\n",
    "                   [-1,-1,-1]])\n",
    "        img = cv2.filter2D(img, -1, kernel) # applying the sharpening kernel.\n",
    "        Result = Sample_ID.isin([id])\n",
    "        Result = Sample_ID[Result];\n",
    "        if len(Result) >= 1:\n",
    "            if len(Result) > 1:  \n",
    "                Result = Sample_ID[Sample_ID==Result.iloc[1]].index.tolist()\n",
    "                Id_1_index = Result[0]\n",
    "                Id_2_index = Result[1]\n",
    "                print(id)\n",
    "                Labels.append([filename,img,(P_Value[Id_1_index]+P_Value[Id_2_index])/2])\n",
    "            else:\n",
    "                Result = Sample_ID[Sample_ID==Result.iloc[1]].index.tolist()\n",
    "                Id_1_index = Result[0]\n",
    "                print(id)\n",
    "                Labels.append([filename,img,(P_Value[Id_1_index])])\n",
    "        else:\n",
    "            print(len(Result))\n",
    "            continue\n",
    "    return Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347046a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110638.0\n",
      "110638.0\n",
      "110639.0\n",
      "110639.0\n",
      "110640.0\n",
      "110640.0\n",
      "110641.0\n",
      "110641.0\n",
      "110642.0\n",
      "110642.0\n",
      "110643.0\n",
      "110643.0\n",
      "110644.0\n",
      "110644.0\n",
      "110645.0\n",
      "110645.0\n",
      "110646.0\n",
      "110646.0\n",
      "110648.0\n",
      "110648.0\n",
      "110649.0\n",
      "110649.0\n",
      "110650.0\n",
      "110650.0\n",
      "110651.0\n",
      "110651.0\n",
      "110652.0\n",
      "110652.0\n",
      "110653.0\n",
      "110653.0\n",
      "110654.0\n",
      "110654.0\n",
      "110655.0\n",
      "110655.0\n",
      "110656.0\n",
      "110656.0\n",
      "110657.0\n",
      "110657.0\n",
      "110658.0\n",
      "110658.0\n",
      "110659.0\n",
      "110659.0\n",
      "110660.0\n",
      "110660.0\n",
      "110661.0\n",
      "110661.0\n",
      "110662.0\n",
      "110662.0\n",
      "110663.0\n",
      "110663.0\n",
      "110664.0\n",
      "110664.0\n",
      "110665.0\n",
      "110665.0\n",
      "110667.0\n",
      "110667.0\n",
      "110734.0\n",
      "110734.0\n",
      "110735.0\n",
      "110735.0\n",
      "110736.0\n",
      "110736.0\n",
      "110737.0\n",
      "110737.0\n",
      "110738.0\n",
      "110738.0\n",
      "110740.0\n",
      "110740.0\n",
      "110745.0\n",
      "110745.0\n",
      "110746.0\n",
      "110746.0\n",
      "110747.0\n",
      "110747.0\n",
      "110749.0\n",
      "110749.0\n",
      "110750.0\n",
      "110750.0\n",
      "110752.0\n",
      "110752.0\n",
      "110753.0\n",
      "110753.0\n",
      "110755.0\n",
      "110755.0\n",
      "110756.0\n",
      "110756.0\n",
      "110757.0\n",
      "110757.0\n",
      "110759.0\n",
      "110759.0\n",
      "110760.0\n",
      "110760.0\n",
      "110761.0\n",
      "110761.0\n",
      "110762.0\n",
      "110762.0\n",
      "110763.0\n",
      "110763.0\n",
      "110766.0\n",
      "110766.0\n",
      "110767.0\n",
      "110767.0\n",
      "110768.0\n",
      "110768.0\n",
      "110773.0\n",
      "110773.0\n",
      "110775.0\n",
      "110775.0\n",
      "110776.0\n",
      "110776.0\n",
      "110777.0\n",
      "110777.0\n",
      "110778.0\n",
      "110778.0\n",
      "110779.0\n",
      "110779.0\n",
      "110781.0\n",
      "110781.0\n",
      "110852.0\n",
      "110852.0\n",
      "110853.0\n",
      "110853.0\n",
      "110854.0\n",
      "110854.0\n",
      "110855.0\n",
      "110855.0\n",
      "110856.0\n",
      "110856.0\n",
      "110857.0\n",
      "110857.0\n",
      "110858.0\n",
      "110858.0\n",
      "110859.0\n",
      "110859.0\n",
      "110860.0\n",
      "110860.0\n",
      "110862.0\n",
      "110862.0\n",
      "110863.0\n",
      "110863.0\n",
      "110866.0\n",
      "110866.0\n",
      "110867.0\n",
      "110867.0\n",
      "110868.0\n",
      "110868.0\n",
      "110869.0\n",
      "110869.0\n",
      "110870.0\n",
      "110870.0\n",
      "110872.0\n",
      "110872.0\n",
      "110873.0\n",
      "110873.0\n",
      "110882.0\n",
      "110882.0\n",
      "110883.0\n",
      "110883.0\n",
      "110884.0\n",
      "110884.0\n",
      "110886.0\n",
      "110886.0\n",
      "110892.0\n",
      "110892.0\n",
      "110893.0\n",
      "110893.0\n",
      "110895.0\n",
      "110895.0\n",
      "110897.0\n",
      "110897.0\n",
      "110898.0\n",
      "110898.0\n",
      "110899.0\n",
      "110899.0\n",
      "110900.0\n",
      "110900.0\n",
      "110902.0\n",
      "110902.0\n",
      "110903.0\n",
      "110903.0\n",
      "110904.0\n",
      "110904.0\n",
      "110905.0\n",
      "110905.0\n",
      "110983.0\n",
      "110983.0\n",
      "110984.0\n",
      "110984.0\n",
      "110986.0\n",
      "110986.0\n",
      "110987.0\n",
      "110987.0\n",
      "110988.0\n",
      "110988.0\n",
      "110989.0\n",
      "110989.0\n",
      "110990.0\n",
      "110990.0\n",
      "110991.0\n",
      "110991.0\n",
      "110992.0\n",
      "110992.0\n",
      "110992.0\n",
      "110992.0\n",
      "110993.0\n",
      "110993.0\n",
      "110994.0\n",
      "110994.0\n",
      "110995.0\n",
      "110995.0\n",
      "110997.0\n",
      "110997.0\n",
      "111000.0\n",
      "111000.0\n",
      "111001.0\n",
      "111001.0\n",
      "111003.0\n",
      "111003.0\n",
      "111004.0\n",
      "111004.0\n",
      "111016.0\n",
      "111016.0\n",
      "111017.0\n",
      "111017.0\n",
      "111018.0\n",
      "111018.0\n",
      "111019.0\n",
      "111019.0\n",
      "111021.0\n",
      "111021.0\n",
      "111021.0\n",
      "111021.0\n",
      "111023.0\n",
      "111023.0\n",
      "111024.0\n",
      "111024.0\n",
      "111024.0\n",
      "111024.0\n",
      "122630.0\n",
      "122630.0\n",
      "122737.0\n",
      "122741.0\n",
      "122741.0\n",
      "122746.0\n",
      "122746.0\n",
      "122753.0\n",
      "122753.0\n",
      "122761.0\n",
      "122761.0\n",
      "122790.0\n",
      "122790.0\n",
      "122792.0\n",
      "122792.0\n",
      "122847.0\n",
      "122847.0\n",
      "122879.0\n",
      "122879.0\n",
      "122906.0\n",
      "122906.0\n",
      "122907.0\n",
      "122907.0\n",
      "122911.0\n",
      "122911.0\n",
      "122922.0\n",
      "122922.0\n",
      "122934.0\n",
      "122935.0\n",
      "122935.0\n",
      "122936.0\n",
      "122937.0\n",
      "122938.0\n",
      "122938.0\n",
      "122939.0\n",
      "122939.0\n",
      "122941.0\n",
      "122941.0\n",
      "122947.0\n",
      "122947.0\n",
      "122963.0\n",
      "122963.0\n",
      "122964.0\n",
      "122964.0\n",
      "123004.0\n",
      "123004.0\n",
      "123044.0\n",
      "123045.0\n",
      "123064.0\n",
      "123570.0\n",
      "123570.0\n",
      "123571.0\n",
      "123571.0\n",
      "123572.0\n",
      "123572.0\n",
      "123573.0\n",
      "123573.0\n",
      "123574.0\n",
      "123574.0\n",
      "123575.0\n",
      "123575.0\n",
      "123576.0\n",
      "123576.0\n",
      "123577.0\n",
      "123577.0\n",
      "123578.0\n",
      "123578.0\n",
      "123579.0\n",
      "123579.0\n",
      "123580.0\n",
      "123580.0\n",
      "123581.0\n",
      "123581.0\n",
      "123582.0\n",
      "123582.0\n",
      "123583.0\n",
      "123583.0\n",
      "123584.0\n",
      "123584.0\n",
      "123585.0\n",
      "123585.0\n",
      "123586.0\n",
      "123586.0\n",
      "123678.0\n",
      "123678.0\n",
      "123680.0\n",
      "123680.0\n",
      "123681.0\n",
      "123681.0\n",
      "123682.0\n",
      "123682.0\n",
      "123683.0\n",
      "123683.0\n",
      "123684.0\n",
      "123684.0\n",
      "123685.0\n",
      "123685.0\n",
      "123686.0\n",
      "123686.0\n",
      "123688.0\n",
      "123688.0\n",
      "123689.0\n",
      "123689.0\n",
      "123691.0\n",
      "123691.0\n",
      "123692.0\n",
      "123692.0\n",
      "123694.0\n",
      "123694.0\n",
      "123695.0\n",
      "123695.0\n",
      "123696.0\n",
      "123696.0\n",
      "123697.0\n",
      "123697.0\n",
      "123698.0\n",
      "123698.0\n",
      "123699.0\n",
      "123699.0\n",
      "123700.0\n",
      "123700.0\n",
      "123701.0\n",
      "123701.0\n",
      "123702.0\n",
      "123702.0\n",
      "123703.0\n",
      "123703.0\n",
      "123704.0\n",
      "123704.0\n",
      "123705.0\n",
      "123705.0\n",
      "123706.0\n",
      "123706.0\n",
      "123707.0\n",
      "123707.0\n",
      "123708.0\n",
      "123708.0\n",
      "123709.0\n",
      "123709.0\n",
      "123710.0\n",
      "123710.0\n",
      "123711.0\n",
      "123711.0\n",
      "123712.0\n",
      "123712.0\n",
      "123713.0\n",
      "123713.0\n",
      "123714.0\n",
      "123714.0\n",
      "123718.0\n",
      "123718.0\n",
      "123719.0\n",
      "123719.0\n",
      "123720.0\n",
      "123720.0\n",
      "123721.0\n",
      "123721.0\n",
      "123722.0\n",
      "123722.0\n",
      "123723.0\n",
      "123723.0\n",
      "123724.0\n",
      "123724.0\n",
      "123725.0\n",
      "123725.0\n",
      "123893.0\n",
      "123893.0\n",
      "123911.0\n",
      "123911.0\n",
      "123916.0\n",
      "123916.0\n",
      "123917.0\n",
      "123917.0\n",
      "123918.0\n",
      "123918.0\n",
      "123919.0\n",
      "123919.0\n",
      "123920.0\n",
      "123920.0\n",
      "123921.0\n",
      "123921.0\n",
      "123922.0\n",
      "123922.0\n",
      "123923.0\n",
      "123923.0\n",
      "123924.0\n",
      "123924.0\n",
      "123925.0\n",
      "123925.0\n",
      "123926.0\n",
      "123926.0\n",
      "123927.0\n",
      "123927.0\n",
      "123928.0\n",
      "123928.0\n",
      "123930.0\n",
      "123930.0\n",
      "123931.0\n",
      "123931.0\n",
      "123932.0\n",
      "123932.0\n",
      "123933.0\n",
      "123933.0\n",
      "123934.0\n",
      "123934.0\n",
      "123935.0\n",
      "123935.0\n",
      "123936.0\n",
      "123936.0\n",
      "123937.0\n",
      "123937.0\n",
      "123938.0\n",
      "123938.0\n",
      "123939.0\n",
      "123939.0\n",
      "123940.0\n",
      "123940.0\n",
      "123941.0\n",
      "123941.0\n",
      "123942.0\n",
      "123942.0\n",
      "123944.0\n",
      "123944.0\n",
      "123945.0\n",
      "123945.0\n",
      "123946.0\n",
      "123946.0\n",
      "123947.0\n",
      "123947.0\n",
      "123948.0\n",
      "123948.0\n",
      "123949.0\n",
      "123949.0\n",
      "123950.0\n",
      "123950.0\n",
      "123951.0\n",
      "123951.0\n",
      "123952.0\n",
      "123952.0\n",
      "123953.0\n",
      "123953.0\n",
      "123954.0\n",
      "123954.0\n",
      "123955.0\n",
      "123955.0\n",
      "123956.0\n",
      "123956.0\n",
      "123957.0\n",
      "123957.0\n",
      "123958.0\n",
      "123958.0\n",
      "123959.0\n",
      "123959.0\n",
      "123960.0\n",
      "123960.0\n",
      "123961.0\n",
      "123961.0\n",
      "123962.0\n",
      "123962.0\n",
      "123963.0\n",
      "123963.0\n",
      "123964.0\n",
      "123964.0\n",
      "123965.0\n",
      "123965.0\n",
      "123967.0\n",
      "123967.0\n",
      "123968.0\n",
      "123968.0\n",
      "123969.0\n",
      "123969.0\n",
      "123970.0\n",
      "123970.0\n",
      "123971.0\n",
      "123971.0\n",
      "123972.0\n",
      "123972.0\n",
      "123973.0\n",
      "123973.0\n",
      "123974.0\n",
      "123974.0\n",
      "123975.0\n",
      "123975.0\n",
      "123976.0\n",
      "123976.0\n",
      "123977.0\n",
      "123977.0\n",
      "123978.0\n",
      "123978.0\n",
      "123979.0\n",
      "123979.0\n",
      "123980.0\n",
      "123980.0\n",
      "123981.0\n",
      "123981.0\n",
      "123981.0\n",
      "123982.0\n",
      "123982.0\n",
      "123983.0\n",
      "123983.0\n",
      "123984.0\n",
      "123984.0\n",
      "123985.0\n",
      "123985.0\n",
      "123986.0\n",
      "123986.0\n",
      "123987.0\n",
      "123987.0\n",
      "123988.0\n",
      "123988.0\n",
      "123989.0\n",
      "123989.0\n",
      "123990.0\n",
      "123990.0\n",
      "123992.0\n",
      "123992.0\n",
      "123998.0\n",
      "123998.0\n",
      "123999.0\n",
      "123999.0\n",
      "124000.0\n",
      "124000.0\n",
      "124001.0\n",
      "124001.0\n",
      "124002.0\n",
      "124002.0\n",
      "124003.0\n",
      "124003.0\n",
      "124004.0\n",
      "124004.0\n",
      "124005.0\n",
      "124005.0\n",
      "124006.0\n",
      "124006.0\n",
      "124007.0\n",
      "124007.0\n",
      "124007.0\n",
      "124008.0\n",
      "124008.0\n",
      "124009.0\n",
      "124009.0\n",
      "124182.0\n",
      "124182.0\n",
      "124183.0\n",
      "124183.0\n",
      "124184.0\n",
      "124184.0\n",
      "124185.0\n",
      "124185.0\n",
      "124186.0\n",
      "124186.0\n",
      "124187.0\n",
      "124187.0\n",
      "124188.0\n",
      "124188.0\n",
      "124189.0\n",
      "124189.0\n",
      "124190.0\n",
      "124190.0\n",
      "124191.0\n",
      "124191.0\n",
      "124191.0\n",
      "124191.0\n",
      "124192.0\n",
      "124192.0\n",
      "124192.0\n",
      "124192.0\n",
      "124193.0\n",
      "124193.0\n",
      "124194.0\n",
      "124194.0\n",
      "124195.0\n",
      "124195.0\n",
      "124196.0\n",
      "124196.0\n",
      "124197.0\n",
      "124197.0\n",
      "124198.0\n",
      "124198.0\n",
      "124199.0\n",
      "124199.0\n",
      "124200.0\n",
      "124200.0\n",
      "124201.0\n",
      "124201.0\n",
      "124202.0\n",
      "124202.0\n",
      "124203.0\n",
      "124203.0\n",
      "124205.0\n",
      "124205.0\n",
      "124206.0\n",
      "124206.0\n",
      "124207.0\n",
      "124208.0\n",
      "124208.0\n",
      "124209.0\n",
      "124209.0\n",
      "124210.0\n",
      "124210.0\n",
      "124211.0\n",
      "124211.0\n",
      "124212.0\n",
      "124212.0\n",
      "124213.0\n",
      "124213.0\n",
      "124214.0\n",
      "124214.0\n",
      "124215.0\n",
      "124215.0\n",
      "124216.0\n",
      "124216.0\n",
      "124217.0\n",
      "124217.0\n",
      "124218.0\n",
      "124218.0\n",
      "124219.0\n",
      "124219.0\n",
      "124220.0\n",
      "124220.0\n",
      "124221.0\n",
      "124221.0\n",
      "124222.0\n",
      "124222.0\n",
      "124223.0\n",
      "124223.0\n",
      "124224.0\n",
      "124224.0\n",
      "124225.0\n",
      "124225.0\n",
      "124226.0\n",
      "124226.0\n",
      "124227.0\n",
      "124227.0\n",
      "124228.0\n",
      "124228.0\n",
      "124229.0\n",
      "124229.0\n",
      "124230.0\n",
      "124230.0\n",
      "124231.0\n",
      "124231.0\n",
      "124232.0\n",
      "124232.0\n",
      "124233.0\n",
      "124233.0\n",
      "124234.0\n",
      "124234.0\n",
      "124235.0\n",
      "124235.0\n",
      "124236.0\n",
      "124237.0\n",
      "124238.0\n",
      "124238.0\n",
      "124239.0\n",
      "124240.0\n",
      "124241.0\n",
      "124242.0\n",
      "124242.0\n",
      "124243.0\n",
      "124243.0\n",
      "124244.0\n",
      "124244.0\n"
     ]
    }
   ],
   "source": [
    "dataset = load_images_from_folder('C:\\\\Users\\\\Mahmood Yousaf\\\\Desktop\\\\FYP\\\\Dataset RGB\\\\testing_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed3469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.4, 5.4, 4.9, 4.9, 4.199999999999999, 4.199999999999999, 5.15, 5.15, 5.300000000000001, 5.300000000000001, 4.300000000000001, 4.300000000000001, 4.95, 4.95, 4.55, 4.55, 4.35, 4.35, 4.15, 4.15, 4.1, 4.1, 5.05, 5.05, 5.15, 5.15, 3.75, 3.75, 4.35, 4.35, 4.05, 4.05, 4.15, 4.15, 4.1, 4.1, 5.1, 5.1, 4.1, 4.1, 3.8499999999999996, 3.8499999999999996, 5.15, 5.15, 3.45, 3.45, 4.45, 4.45, 3.15, 3.15, 4.449999999999999, 4.449999999999999, 4.55, 4.55, 4.25, 4.25, 4.699999999999999, 4.699999999999999, 4.1, 4.1, 4.35, 4.35, 4.449999999999999, 4.449999999999999, 4.0, 4.0, 3.35, 3.35, 5.1, 5.1, 4.6, 4.6, 4.300000000000001, 4.300000000000001, 5.75, 5.75, 4.55, 4.55, 5.05, 5.05, 6.6, 6.6, 4.55, 4.55, 4.6, 4.6, 4.4, 4.4, 4.35, 4.35, 4.35, 4.35, 5.2, 5.2, 4.05, 4.05, 3.8, 3.8, 3.35, 3.35, 4.65, 4.65, 5.3, 5.3, 3.95, 3.95, 5.15, 5.15, 4.35, 4.35, 4.949999999999999, 4.949999999999999, 4.300000000000001, 4.300000000000001, 4.25, 4.25, 4.35, 4.35, 4.6, 4.6, 5.65, 5.65, 4.65, 4.65, 4.1, 4.1, 4.4, 4.4, 5.35, 5.35, 4.5, 4.5, 3.25, 3.25, 4.15, 4.15, 4.05, 4.05, 4.0, 4.0, 5.949999999999999, 5.949999999999999, 4.9, 4.9, 8.65, 8.65, 7.45, 7.45, 5.15, 5.15, 5.15, 5.15, 6.5, 6.5, 4.3, 4.3, 6.3999999999999995, 6.3999999999999995, 5.4, 5.4, 4.199999999999999, 4.199999999999999, 3.7, 3.7, 3.65, 3.65, 3.55, 3.55, 5.85, 5.85, 3.1500000000000004, 3.1500000000000004, 4.05, 4.05, 4.45, 4.45, 4.25, 4.25, 3.9000000000000004, 3.9000000000000004, 3.9, 3.9, 3.3, 3.3, 4.5, 4.5, 5.3, 5.3, 5.35, 5.35, 5.2, 5.2, 4.949999999999999, 4.949999999999999, 4.15, 4.15, 3.5, 3.5, 3.25, 3.25, 6.15, 6.15, 6.15, 6.15, 3.05, 3.05, 4.4, 4.4, 5.65, 5.65, 4.4, 4.4, 5.65, 5.65, 4.05, 4.05, 4.9, 4.9, 3.95, 3.95, 7.5, 7.5, 5.55, 5.55, 4.800000000000001, 4.800000000000001, 5.35, 5.35, 4.05, 4.05, 4.05, 4.05, 5.300000000000001, 5.300000000000001, 5.1, 5.1, 5.1, 5.1, 6.1, 6.1, 3.35, 3.35, 3.35, 8.15, 8.15, 7.4, 7.4, 6.35, 6.35, 4.75, 4.75, 7.2, 7.2, 5.25, 5.25, 5.25, 5.25, 5.9, 5.9, 7.75, 7.75, 6.75, 6.75, 4.75, 4.75, 8.2, 3.85, 3.85, 10.1, 5.550000000000001, 5.8, 5.8, 8.45, 8.45, 8.65, 8.65, 6.1, 6.1, 7.1, 7.1, 8.25, 8.25, 8.25, 8.25, 8.3, 5.1, 6.35, 4.4, 4.4, 4.699999999999999, 4.699999999999999, 5.1, 5.1, 4.75, 4.75, 5.1, 5.1, 4.6, 4.6, 3.8499999999999996, 3.8499999999999996, 3.45, 3.45, 3.35, 3.35, 3.5, 3.5, 3.35, 3.35, 4.1, 4.1, 4.35, 4.35, 4.550000000000001, 4.550000000000001, 4.35, 4.35, 4.6, 4.6, 5.050000000000001, 5.050000000000001, 5.25, 5.25, 4.65, 4.65, 4.05, 4.05, 5.05, 5.05, 5.15, 5.15, 5.25, 5.25, 4.449999999999999, 4.449999999999999, 4.35, 4.35, 4.199999999999999, 4.199999999999999, 4.05, 4.05, 4.3, 4.3, 4.35, 4.35, 5.05, 5.05, 4.300000000000001, 4.300000000000001, 5.5, 5.5, 4.35, 4.35, 5.75, 5.75, 5.1, 5.1, 5.5, 5.5, 4.9, 4.9, 4.35, 4.35, 4.15, 4.15, 3.6, 3.6, 4.05, 4.05, 4.65, 4.65, 4.05, 4.05, 4.65, 4.65, 4.15, 4.15, 4.75, 4.75, 5.05, 5.05, 4.949999999999999, 4.949999999999999, 3.9, 3.9, 5.1, 5.1, 3.9, 3.9, 4.15, 4.15, 4.3, 4.3, 6.199999999999999, 6.199999999999999, 6.25, 6.25, 5.699999999999999, 5.699999999999999, 6.4, 6.4, 6.75, 6.75, 5.55, 5.55, 4.65, 4.65, 4.65, 4.65, 4.949999999999999, 4.949999999999999, 3.4000000000000004, 3.4000000000000004, 3.45, 3.45, 4.8, 4.8, 4.15, 4.15, 4.4, 4.4, 5.0, 5.0, 4.9, 4.9, 4.5, 4.5, 6.6, 6.6, 6.25, 6.25, 6.1, 6.1, 5.05, 5.05, 5.4, 5.4, 4.9, 4.9, 5.4, 5.4, 4.6, 4.6, 5.3, 5.3, 5.55, 5.55, 6.35, 6.35, 5.25, 5.25, 5.35, 5.35, 4.949999999999999, 4.949999999999999, 5.25, 5.25, 5.35, 5.35, 3.1, 3.1, 5.15, 5.15, 5.85, 5.85, 5.85, 5.85, 5.699999999999999, 5.699999999999999, 5.55, 5.55, 5.35, 5.35, 5.5, 5.5, 4.85, 4.85, 5.1, 5.1, 5.95, 5.95, 7.0, 7.0, 5.1, 5.1, 4.55, 4.55, 4.75, 4.75, 5.35, 5.35, 5.1, 5.1, 4.45, 4.45, 5.1, 5.1, 5.25, 5.25, 4.35, 4.35, 4.6, 4.6, 6.4, 6.4, 5.65, 5.65, 4.85, 4.85, 4.75, 4.75, 5.45, 5.45, 4.8, 4.8, 5.85, 5.85, 5.300000000000001, 5.300000000000001, 4.65, 4.65, 5.4, 5.4, 4.1, 4.1, 5.449999999999999, 5.449999999999999, 4.15, 4.15, 4.699999999999999, 4.699999999999999, 4.6, 4.6, 4.6, 4.55, 4.55, 4.65, 4.65, 4.95, 4.95, 4.1, 4.1, 4.35, 4.35, 4.15, 4.15, 5.15, 5.15, 5.1, 5.1, 4.1, 4.1, 4.25, 4.25, 4.449999999999999, 4.449999999999999, 4.55, 4.55, 4.699999999999999, 4.699999999999999, 5.1, 5.1, 4.300000000000001, 4.300000000000001, 5.2, 5.2, 4.55, 4.55, 5.45, 5.45, 6.800000000000001, 6.800000000000001, 6.8500000000000005, 6.8500000000000005, 6.8500000000000005, 5.1, 5.1, 5.5, 5.5, 4.15, 4.15, 4.4, 4.4, 4.6, 4.6, 5.3, 5.3, 4.75, 4.75, 4.65, 4.65, 6.5, 6.5, 4.05, 4.05, 5.199999999999999, 5.199999999999999, 8.7, 8.7, 8.7, 8.7, 6.699999999999999, 6.699999999999999, 6.699999999999999, 6.699999999999999, 5.300000000000001, 5.300000000000001, 5.1, 5.1, 5.55, 5.55, 5.1, 5.1, 5.300000000000001, 5.300000000000001, 4.35, 4.35, 4.6, 4.6, 4.45, 4.45, 5.300000000000001, 5.300000000000001, 4.25, 4.25, 4.4, 4.4, 4.4, 4.4, 4.8, 4.8, 4.8, 4.95, 4.95, 4.85, 4.85, 4.75, 4.75, 4.8, 4.8, 5.35, 5.35, 4.3, 4.3, 4.7, 4.7, 4.5, 4.5, 4.949999999999999, 4.949999999999999, 4.7, 4.7, 7.85, 7.85, 5.0, 5.0, 6.15, 6.15, 4.800000000000001, 4.800000000000001, 4.9, 4.9, 7.65, 7.65, 5.4, 5.4, 4.800000000000001, 4.800000000000001, 5.3, 5.3, 6.35, 6.35, 5.55, 5.55, 5.25, 5.25, 4.949999999999999, 4.949999999999999, 5.25, 5.25, 5.25, 5.25, 5.5, 5.5, 5.35, 5.35, 5.1, 5.1, 4.9, 5.35, 4.1, 4.1, 5.449999999999999, 4.15, 4.699999999999999, 4.55, 4.55, 5.1, 5.1, 5.35, 5.35]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for data in dataset:\n",
    "    X.append(data[1])\n",
    "    Y.append(data[2])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1081fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300, 3)\n",
      "[ 5.4   5.4   4.9   4.9   4.2   4.2   5.15  5.15  5.3   5.3   4.3   4.3\n",
      "  4.95  4.95  4.55  4.55  4.35  4.35  4.15  4.15  4.1   4.1   5.05  5.05\n",
      "  5.15  5.15  3.75  3.75  4.35  4.35  4.05  4.05  4.15  4.15  4.1   4.1\n",
      "  5.1   5.1   4.1   4.1   3.85  3.85  5.15  5.15  3.45  3.45  4.45  4.45\n",
      "  3.15  3.15  4.45  4.45  4.55  4.55  4.25  4.25  4.7   4.7   4.1   4.1\n",
      "  4.35  4.35  4.45  4.45  4.    4.    3.35  3.35  5.1   5.1   4.6   4.6\n",
      "  4.3   4.3   5.75  5.75  4.55  4.55  5.05  5.05  6.6   6.6   4.55  4.55\n",
      "  4.6   4.6   4.4   4.4   4.35  4.35  4.35  4.35  5.2   5.2   4.05  4.05\n",
      "  3.8   3.8   3.35  3.35  4.65  4.65  5.3   5.3   3.95  3.95  5.15  5.15\n",
      "  4.35  4.35  4.95  4.95  4.3   4.3   4.25  4.25  4.35  4.35  4.6   4.6\n",
      "  5.65  5.65  4.65  4.65  4.1   4.1   4.4   4.4   5.35  5.35  4.5   4.5\n",
      "  3.25  3.25  4.15  4.15  4.05  4.05  4.    4.    5.95  5.95  4.9   4.9\n",
      "  8.65  8.65  7.45  7.45  5.15  5.15  5.15  5.15  6.5   6.5   4.3   4.3\n",
      "  6.4   6.4   5.4   5.4   4.2   4.2   3.7   3.7   3.65  3.65  3.55  3.55\n",
      "  5.85  5.85  3.15  3.15  4.05  4.05  4.45  4.45  4.25  4.25  3.9   3.9\n",
      "  3.9   3.9   3.3   3.3   4.5   4.5   5.3   5.3   5.35  5.35  5.2   5.2\n",
      "  4.95  4.95  4.15  4.15  3.5   3.5   3.25  3.25  6.15  6.15  6.15  6.15\n",
      "  3.05  3.05  4.4   4.4   5.65  5.65  4.4   4.4   5.65  5.65  4.05  4.05\n",
      "  4.9   4.9   3.95  3.95  7.5   7.5   5.55  5.55  4.8   4.8   5.35  5.35\n",
      "  4.05  4.05  4.05  4.05  5.3   5.3   5.1   5.1   5.1   5.1   6.1   6.1\n",
      "  3.35  3.35  3.35  8.15  8.15  7.4   7.4   6.35  6.35  4.75  4.75  7.2\n",
      "  7.2   5.25  5.25  5.25  5.25  5.9   5.9   7.75  7.75  6.75  6.75  4.75\n",
      "  4.75  8.2   3.85  3.85 10.1   5.55  5.8   5.8   8.45  8.45  8.65  8.65\n",
      "  6.1   6.1   7.1   7.1   8.25  8.25  8.25  8.25  8.3   5.1   6.35  4.4\n",
      "  4.4   4.7   4.7   5.1   5.1   4.75  4.75  5.1   5.1   4.6   4.6   3.85\n",
      "  3.85  3.45  3.45  3.35  3.35  3.5   3.5   3.35  3.35  4.1   4.1   4.35\n",
      "  4.35  4.55  4.55  4.35  4.35  4.6   4.6   5.05  5.05  5.25  5.25  4.65\n",
      "  4.65  4.05  4.05  5.05  5.05  5.15  5.15  5.25  5.25  4.45  4.45  4.35\n",
      "  4.35  4.2   4.2   4.05  4.05  4.3   4.3   4.35  4.35  5.05  5.05  4.3\n",
      "  4.3   5.5   5.5   4.35  4.35  5.75  5.75  5.1   5.1   5.5   5.5   4.9\n",
      "  4.9   4.35  4.35  4.15  4.15  3.6   3.6   4.05  4.05  4.65  4.65  4.05\n",
      "  4.05  4.65  4.65  4.15  4.15  4.75  4.75  5.05  5.05  4.95  4.95  3.9\n",
      "  3.9   5.1   5.1   3.9   3.9   4.15  4.15  4.3   4.3   6.2   6.2   6.25\n",
      "  6.25  5.7   5.7   6.4   6.4   6.75  6.75  5.55  5.55  4.65  4.65  4.65\n",
      "  4.65  4.95  4.95  3.4   3.4   3.45  3.45  4.8   4.8   4.15  4.15  4.4\n",
      "  4.4   5.    5.    4.9   4.9   4.5   4.5   6.6   6.6   6.25  6.25  6.1\n",
      "  6.1   5.05  5.05  5.4   5.4   4.9   4.9   5.4   5.4   4.6   4.6   5.3\n",
      "  5.3   5.55  5.55  6.35  6.35  5.25  5.25  5.35  5.35  4.95  4.95  5.25\n",
      "  5.25  5.35  5.35  3.1   3.1   5.15  5.15  5.85  5.85  5.85  5.85  5.7\n",
      "  5.7   5.55  5.55  5.35  5.35  5.5   5.5   4.85  4.85  5.1   5.1   5.95\n",
      "  5.95  7.    7.    5.1   5.1   4.55  4.55  4.75  4.75  5.35  5.35  5.1\n",
      "  5.1   4.45  4.45  5.1   5.1   5.25  5.25  4.35  4.35  4.6   4.6   6.4\n",
      "  6.4   5.65  5.65  4.85  4.85  4.75  4.75  5.45  5.45  4.8   4.8   5.85\n",
      "  5.85  5.3   5.3   4.65  4.65  5.4   5.4   4.1   4.1   5.45  5.45  4.15\n",
      "  4.15  4.7   4.7   4.6   4.6   4.6   4.55  4.55  4.65  4.65  4.95  4.95\n",
      "  4.1   4.1   4.35  4.35  4.15  4.15  5.15  5.15  5.1   5.1   4.1   4.1\n",
      "  4.25  4.25  4.45  4.45  4.55  4.55  4.7   4.7   5.1   5.1   4.3   4.3\n",
      "  5.2   5.2   4.55  4.55  5.45  5.45  6.8   6.8   6.85  6.85  6.85  5.1\n",
      "  5.1   5.5   5.5   4.15  4.15  4.4   4.4   4.6   4.6   5.3   5.3   4.75\n",
      "  4.75  4.65  4.65  6.5   6.5   4.05  4.05  5.2   5.2   8.7   8.7   8.7\n",
      "  8.7   6.7   6.7   6.7   6.7   5.3   5.3   5.1   5.1   5.55  5.55  5.1\n",
      "  5.1   5.3   5.3   4.35  4.35  4.6   4.6   4.45  4.45  5.3   5.3   4.25\n",
      "  4.25  4.4   4.4   4.4   4.4   4.8   4.8   4.8   4.95  4.95  4.85  4.85\n",
      "  4.75  4.75  4.8   4.8   5.35  5.35  4.3   4.3   4.7   4.7   4.5   4.5\n",
      "  4.95  4.95  4.7   4.7   7.85  7.85  5.    5.    6.15  6.15  4.8   4.8\n",
      "  4.9   4.9   7.65  7.65  5.4   5.4   4.8   4.8   5.3   5.3   6.35  6.35\n",
      "  5.55  5.55  5.25  5.25  4.95  4.95  5.25  5.25  5.25  5.25  5.5   5.5\n",
      "  5.35  5.35  5.1   5.1   4.9   5.35  4.1   4.1   5.45  4.15  4.7   4.55\n",
      "  4.55  5.1   5.1   5.35  5.35]\n"
     ]
    }
   ],
   "source": [
    "img_size = 300\n",
    "X = np.array(X).reshape(-1, img_size, img_size, 3)\n",
    "print(X[0].shape)\n",
    "Y = np.array(Y)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7aa3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6843fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AlexNet Architecture for P estimation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(300,300,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c85ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525, 300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, Y)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d90be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debf1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "10/10 [==============================] - 11s 633ms/step - loss: 39135.4844 - mse: 39135.4844 - mae: 80.4013 - val_loss: 983616640.0000 - val_mse: 983616640.0000 - val_mae: 30925.8086\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 566.3678 - mse: 566.3678 - mae: 14.6312 - val_loss: 613566.0000 - val_mse: 613566.0000 - val_mae: 763.8745\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 2s 192ms/step - loss: 427.8490 - mse: 427.8490 - mae: 12.5717 - val_loss: 91879.7656 - val_mse: 91879.7656 - val_mae: 286.3428\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 125.2608 - mse: 125.2608 - mae: 7.2661 - val_loss: 1088500.1250 - val_mse: 1088500.1250 - val_mae: 1015.9080\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 96.2481 - mse: 96.2481 - mae: 6.4677 - val_loss: 3788.8970 - val_mse: 3788.8970 - val_mae: 48.4497\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 2s 203ms/step - loss: 57.7840 - mse: 57.7840 - mae: 4.9149 - val_loss: 54706.6445 - val_mse: 54706.6445 - val_mae: 226.6152\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 32.2494 - mse: 32.2494 - mae: 3.7517 - val_loss: 10110.5225 - val_mse: 10110.5225 - val_mae: 96.7501\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 16.3190 - mse: 16.3190 - mae: 2.8458 - val_loss: 2960.6997 - val_mse: 2960.6997 - val_mae: 51.9205\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 8.0430 - mse: 8.0430 - mae: 2.0469 - val_loss: 3837.9836 - val_mse: 3837.9836 - val_mae: 58.6040\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 6.1447 - mse: 6.1447 - mae: 1.9044 - val_loss: 2981.4646 - val_mse: 2981.4646 - val_mae: 51.1227\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 5.2689 - mse: 5.2689 - mae: 1.8132 - val_loss: 1202.7455 - val_mse: 1202.7455 - val_mae: 31.8827\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 4.5128 - mse: 4.5128 - mae: 1.6442 - val_loss: 644.8389 - val_mse: 644.8389 - val_mae: 22.9088\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 4.0790 - mse: 4.0790 - mae: 1.5993 - val_loss: 751.8601 - val_mse: 751.8601 - val_mae: 24.5754\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 4.4770 - mse: 4.4770 - mae: 1.5817 - val_loss: 328.1104 - val_mse: 328.1104 - val_mae: 15.7454\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 3.4363 - mse: 3.4363 - mae: 1.4609 - val_loss: 142.7880 - val_mse: 142.7880 - val_mae: 9.9875\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 4.1937 - mse: 4.1937 - mae: 1.6137 - val_loss: 50.4767 - val_mse: 50.4767 - val_mae: 5.7492\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 3.9099 - mse: 3.9099 - mae: 1.5527 - val_loss: 36.6926 - val_mse: 36.6926 - val_mae: 4.8449\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 4.2670 - mse: 4.2670 - mae: 1.5804 - val_loss: 12.6006 - val_mse: 12.6006 - val_mae: 2.7396\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 3.3678 - mse: 3.3678 - mae: 1.4607 - val_loss: 20.0531 - val_mse: 20.0531 - val_mae: 3.4974\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 2s 200ms/step - loss: 3.1691 - mse: 3.1691 - mae: 1.3860 - val_loss: 4.1001 - val_mse: 4.1001 - val_mae: 1.5786\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 3.0809 - mse: 3.0809 - mae: 1.3914 - val_loss: 8.3183 - val_mse: 8.3183 - val_mae: 2.2354\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.8711 - mse: 2.8711 - mae: 1.3501 - val_loss: 3.6899 - val_mse: 3.6899 - val_mae: 1.4634\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 3.0241 - mse: 3.0241 - mae: 1.3788 - val_loss: 4.6525 - val_mse: 4.6525 - val_mae: 1.6347\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 3.2791 - mse: 3.2791 - mae: 1.3809 - val_loss: 3.1625 - val_mse: 3.1625 - val_mae: 1.3339\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.6737 - mse: 2.6737 - mae: 1.2596 - val_loss: 3.6657 - val_mse: 3.6657 - val_mae: 1.4087\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.5369 - mse: 2.5369 - mae: 1.2403 - val_loss: 2.8298 - val_mse: 2.8298 - val_mae: 1.2581\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.8559 - mse: 2.8559 - mae: 1.3560 - val_loss: 2.6854 - val_mse: 2.6854 - val_mae: 1.2038\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 2.6145 - mse: 2.6145 - mae: 1.2852 - val_loss: 2.6478 - val_mse: 2.6478 - val_mae: 1.2139\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.6563 - mse: 2.6563 - mae: 1.2834 - val_loss: 2.7034 - val_mse: 2.7034 - val_mae: 1.2332\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.7181 - mse: 2.7181 - mae: 1.3310 - val_loss: 2.3427 - val_mse: 2.3427 - val_mae: 1.1136\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.7819 - mse: 2.7819 - mae: 1.3361 - val_loss: 3.2480 - val_mse: 3.2480 - val_mae: 1.3920\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.8951 - mse: 2.8951 - mae: 1.3266 - val_loss: 2.7600 - val_mse: 2.7600 - val_mae: 1.2436\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.9008 - mse: 2.9008 - mae: 1.3453 - val_loss: 3.2040 - val_mse: 3.2040 - val_mae: 1.3742\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.5930 - mse: 2.5930 - mae: 1.2700 - val_loss: 3.0719 - val_mse: 3.0719 - val_mae: 1.3489\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.6803 - mse: 2.6803 - mae: 1.2999 - val_loss: 2.5473 - val_mse: 2.5473 - val_mae: 1.1855\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.5933 - mse: 2.5933 - mae: 1.2497 - val_loss: 2.5805 - val_mse: 2.5805 - val_mae: 1.2119\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.3800 - mse: 2.3800 - mae: 1.2129 - val_loss: 3.3026 - val_mse: 3.3026 - val_mae: 1.3833\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.7577 - mse: 2.7577 - mae: 1.2998 - val_loss: 2.7153 - val_mse: 2.7153 - val_mae: 1.2309\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.4235 - mse: 2.4235 - mae: 1.2165 - val_loss: 2.7765 - val_mse: 2.7765 - val_mae: 1.2505\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.7236 - mse: 2.7236 - mae: 1.3150 - val_loss: 2.9064 - val_mse: 2.9064 - val_mae: 1.2805\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.4013 - mse: 2.4013 - mae: 1.2075 - val_loss: 2.9331 - val_mse: 2.9331 - val_mae: 1.2944\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.3386 - mse: 2.3386 - mae: 1.1871 - val_loss: 2.7090 - val_mse: 2.7090 - val_mae: 1.2422\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.0161 - mse: 2.0161 - mae: 1.1189 - val_loss: 3.1082 - val_mse: 3.1082 - val_mae: 1.3535\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.1867 - mse: 2.1867 - mae: 1.1596 - val_loss: 2.7404 - val_mse: 2.7404 - val_mae: 1.2453\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.3651 - mse: 2.3651 - mae: 1.2191 - val_loss: 2.8849 - val_mse: 2.8849 - val_mae: 1.2903\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.2044 - mse: 2.2044 - mae: 1.1830 - val_loss: 2.6291 - val_mse: 2.6291 - val_mae: 1.2173\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.3923 - mse: 2.3923 - mae: 1.2128 - val_loss: 2.6321 - val_mse: 2.6321 - val_mae: 1.2206\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.2978 - mse: 2.2978 - mae: 1.1663 - val_loss: 2.9306 - val_mse: 2.9306 - val_mae: 1.3026\n",
      "Epoch 49/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 194ms/step - loss: 2.1021 - mse: 2.1021 - mae: 1.1176 - val_loss: 2.8033 - val_mse: 2.8033 - val_mae: 1.2589\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.1508 - mse: 2.1508 - mae: 1.1287 - val_loss: 2.8688 - val_mse: 2.8688 - val_mae: 1.2858\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.2968 - mse: 2.2968 - mae: 1.1864 - val_loss: 2.5244 - val_mse: 2.5244 - val_mae: 1.1749\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.1396 - mse: 2.1396 - mae: 1.0898 - val_loss: 2.4541 - val_mse: 2.4541 - val_mae: 1.1461\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.4652 - mse: 2.4652 - mae: 1.1784 - val_loss: 2.5061 - val_mse: 2.5061 - val_mae: 1.1532\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.2864 - mse: 2.2864 - mae: 1.1933 - val_loss: 2.5837 - val_mse: 2.5837 - val_mae: 1.1840\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.2681 - mse: 2.2681 - mae: 1.1331 - val_loss: 2.7250 - val_mse: 2.7250 - val_mae: 1.2443\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.0276 - mse: 2.0276 - mae: 1.1286 - val_loss: 2.9887 - val_mse: 2.9887 - val_mae: 1.3146\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 2.1477 - mse: 2.1477 - mae: 1.1271 - val_loss: 2.3925 - val_mse: 2.3925 - val_mae: 1.1212\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.0946 - mse: 2.0946 - mae: 1.1498 - val_loss: 3.3929 - val_mse: 3.3929 - val_mae: 1.4232\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.2466 - mse: 2.2466 - mae: 1.1518 - val_loss: 2.2612 - val_mse: 2.2612 - val_mae: 1.0942\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.9156 - mse: 1.9156 - mae: 1.0599 - val_loss: 2.4451 - val_mse: 2.4451 - val_mae: 1.1423\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.0862 - mse: 2.0862 - mae: 1.1544 - val_loss: 2.1595 - val_mse: 2.1595 - val_mae: 1.0609\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.0214 - mse: 2.0214 - mae: 1.1348 - val_loss: 3.5185 - val_mse: 3.5185 - val_mae: 1.4661\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.8926 - mse: 1.8926 - mae: 1.0834 - val_loss: 2.4796 - val_mse: 2.4796 - val_mae: 1.1594\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.7847 - mse: 1.7847 - mae: 1.0386 - val_loss: 2.2716 - val_mse: 2.2716 - val_mae: 1.0974\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.8796 - mse: 1.8796 - mae: 1.0820 - val_loss: 2.4376 - val_mse: 2.4376 - val_mae: 1.1467\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.0194 - mse: 2.0194 - mae: 1.1436 - val_loss: 2.5031 - val_mse: 2.5031 - val_mae: 1.1691\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.8732 - mse: 1.8732 - mae: 1.0944 - val_loss: 2.2569 - val_mse: 2.2569 - val_mae: 1.0828\n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.8975 - mse: 1.8975 - mae: 1.0891 - val_loss: 2.8368 - val_mse: 2.8368 - val_mae: 1.2592\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.6504 - mse: 1.6504 - mae: 0.9963 - val_loss: 2.4729 - val_mse: 2.4729 - val_mae: 1.1445\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.7018 - mse: 1.7018 - mae: 1.0507 - val_loss: 2.3562 - val_mse: 2.3562 - val_mae: 1.1178\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.9476 - mse: 1.9476 - mae: 1.0895 - val_loss: 2.5136 - val_mse: 2.5136 - val_mae: 1.1440\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.8661 - mse: 1.8661 - mae: 1.0752 - val_loss: 3.3916 - val_mse: 3.3916 - val_mae: 1.4264\n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.7051 - mse: 1.7051 - mae: 1.0136 - val_loss: 2.3806 - val_mse: 2.3806 - val_mae: 1.1216\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.8125 - mse: 1.8125 - mae: 1.0608 - val_loss: 2.2144 - val_mse: 2.2144 - val_mae: 1.0768\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 2.2848 - mse: 2.2848 - mae: 1.1852 - val_loss: 2.8601 - val_mse: 2.8601 - val_mae: 1.2746\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.7545 - mse: 1.7545 - mae: 1.0301 - val_loss: 2.6136 - val_mse: 2.6136 - val_mae: 1.1944\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.6199 - mse: 1.6199 - mae: 1.0037 - val_loss: 2.6768 - val_mse: 2.6768 - val_mae: 1.2081\n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.7022 - mse: 1.7022 - mae: 1.0307 - val_loss: 2.3896 - val_mse: 2.3896 - val_mae: 1.1214\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 2s 200ms/step - loss: 1.6564 - mse: 1.6564 - mae: 0.9968 - val_loss: 2.2438 - val_mse: 2.2438 - val_mae: 1.0812\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 1.8333 - mse: 1.8333 - mae: 1.0697 - val_loss: 2.8211 - val_mse: 2.8211 - val_mae: 1.2687\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.9326 - mse: 1.9326 - mae: 1.0637 - val_loss: 3.0496 - val_mse: 3.0496 - val_mae: 1.3195\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.9159 - mse: 1.9159 - mae: 1.0966 - val_loss: 2.2728 - val_mse: 2.2728 - val_mae: 1.0665\n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.6612 - mse: 1.6612 - mae: 1.0072 - val_loss: 2.7706 - val_mse: 2.7706 - val_mae: 1.2464\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.9363 - mse: 1.9363 - mae: 1.0805 - val_loss: 3.0032 - val_mse: 3.0032 - val_mae: 1.3149\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.6590 - mse: 1.6590 - mae: 1.0156 - val_loss: 2.1380 - val_mse: 2.1380 - val_mae: 1.0519\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.7763 - mse: 1.7763 - mae: 1.0487 - val_loss: 3.1657 - val_mse: 3.1657 - val_mae: 1.3707\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.8766 - mse: 1.8766 - mae: 1.0470 - val_loss: 2.6637 - val_mse: 2.6637 - val_mae: 1.2135\n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3055 - mse: 1.3055 - mae: 0.9036 - val_loss: 2.7335 - val_mse: 2.7335 - val_mae: 1.2135\n",
      "Epoch 89/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.6721 - mse: 1.6721 - mae: 1.0105 - val_loss: 2.3190 - val_mse: 2.3190 - val_mae: 1.0875\n",
      "Epoch 90/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.8426 - mse: 1.8426 - mae: 1.0622 - val_loss: 2.3495 - val_mse: 2.3495 - val_mae: 1.0843\n",
      "Epoch 91/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.8476 - mse: 1.8476 - mae: 1.0643 - val_loss: 3.0125 - val_mse: 3.0125 - val_mae: 1.3068\n",
      "Epoch 92/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.6720 - mse: 1.6720 - mae: 1.0211 - val_loss: 2.3070 - val_mse: 2.3070 - val_mae: 1.0844\n",
      "Epoch 93/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.8198 - mse: 1.8198 - mae: 1.0613 - val_loss: 2.6260 - val_mse: 2.6260 - val_mae: 1.1783\n",
      "Epoch 94/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.8039 - mse: 1.8039 - mae: 1.0554 - val_loss: 2.2320 - val_mse: 2.2320 - val_mae: 1.0529\n",
      "Epoch 95/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.9441 - mse: 1.9441 - mae: 1.0946 - val_loss: 2.1341 - val_mse: 2.1341 - val_mae: 1.0309\n",
      "Epoch 96/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.6266 - mse: 1.6266 - mae: 0.9903 - val_loss: 2.5766 - val_mse: 2.5766 - val_mae: 1.1517\n",
      "Epoch 97/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.4487 - mse: 1.4487 - mae: 0.9452 - val_loss: 2.5657 - val_mse: 2.5657 - val_mae: 1.1308\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 195ms/step - loss: 1.6899 - mse: 1.6899 - mae: 1.0026 - val_loss: 2.1632 - val_mse: 2.1632 - val_mae: 1.0330\n",
      "Epoch 99/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.7275 - mse: 1.7275 - mae: 1.0416 - val_loss: 2.5854 - val_mse: 2.5854 - val_mae: 1.1184\n",
      "Epoch 100/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.7046 - mse: 1.7046 - mae: 1.0127 - val_loss: 2.3318 - val_mse: 2.3318 - val_mae: 1.0619\n",
      "Epoch 101/300\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 1.4906 - mse: 1.4906 - mae: 0.9565 - val_loss: 2.1838 - val_mse: 2.1838 - val_mae: 1.0366\n",
      "Epoch 102/300\n",
      "10/10 [==============================] - 2s 200ms/step - loss: 1.4823 - mse: 1.4823 - mae: 0.9475 - val_loss: 3.3385 - val_mse: 3.3385 - val_mae: 1.3913\n",
      "Epoch 103/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.6868 - mse: 1.6868 - mae: 1.0120 - val_loss: 2.2116 - val_mse: 2.2116 - val_mae: 1.0593\n",
      "Epoch 104/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.6559 - mse: 1.6559 - mae: 1.0215 - val_loss: 2.5210 - val_mse: 2.5210 - val_mae: 1.1446\n",
      "Epoch 105/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.6201 - mse: 1.6201 - mae: 0.9684 - val_loss: 2.5124 - val_mse: 2.5124 - val_mae: 1.1387\n",
      "Epoch 106/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.4346 - mse: 1.4346 - mae: 0.9138 - val_loss: 2.4732 - val_mse: 2.4732 - val_mae: 1.1142\n",
      "Epoch 107/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.6283 - mse: 1.6283 - mae: 1.0104 - val_loss: 2.4066 - val_mse: 2.4066 - val_mae: 1.1027\n",
      "Epoch 108/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.5537 - mse: 1.5537 - mae: 0.9611 - val_loss: 2.1881 - val_mse: 2.1881 - val_mae: 1.0639\n",
      "Epoch 109/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.4944 - mse: 1.4944 - mae: 0.9485 - val_loss: 2.6536 - val_mse: 2.6536 - val_mae: 1.1866\n",
      "Epoch 110/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.6742 - mse: 1.6742 - mae: 1.0149 - val_loss: 3.3409 - val_mse: 3.3409 - val_mae: 1.3681\n",
      "Epoch 111/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.7883 - mse: 1.7883 - mae: 1.0478 - val_loss: 2.9296 - val_mse: 2.9296 - val_mae: 1.2549\n",
      "Epoch 112/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3578 - mse: 1.3578 - mae: 0.9235 - val_loss: 2.7094 - val_mse: 2.7094 - val_mae: 1.1958\n",
      "Epoch 113/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.7229 - mse: 1.7229 - mae: 1.0345 - val_loss: 2.7255 - val_mse: 2.7255 - val_mae: 1.1893\n",
      "Epoch 114/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4623 - mse: 1.4623 - mae: 0.9431 - val_loss: 2.6709 - val_mse: 2.6709 - val_mae: 1.1813\n",
      "Epoch 115/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4189 - mse: 1.4189 - mae: 0.9329 - val_loss: 2.3485 - val_mse: 2.3485 - val_mae: 1.0802\n",
      "Epoch 116/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4617 - mse: 1.4617 - mae: 0.9469 - val_loss: 2.6340 - val_mse: 2.6340 - val_mae: 1.1395\n",
      "Epoch 117/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3257 - mse: 1.3257 - mae: 0.9117 - val_loss: 2.5602 - val_mse: 2.5602 - val_mae: 1.1377\n",
      "Epoch 118/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4665 - mse: 1.4665 - mae: 0.9500 - val_loss: 2.6993 - val_mse: 2.6993 - val_mae: 1.1668\n",
      "Epoch 119/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3672 - mse: 1.3672 - mae: 0.9483 - val_loss: 3.4790 - val_mse: 3.4790 - val_mae: 1.4264\n",
      "Epoch 120/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.5980 - mse: 1.5980 - mae: 0.9959 - val_loss: 2.9396 - val_mse: 2.9396 - val_mae: 1.2691\n",
      "Epoch 121/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3371 - mse: 1.3371 - mae: 0.9209 - val_loss: 2.3047 - val_mse: 2.3047 - val_mae: 1.0786\n",
      "Epoch 122/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3707 - mse: 1.3707 - mae: 0.9157 - val_loss: 2.4692 - val_mse: 2.4692 - val_mae: 1.1154\n",
      "Epoch 123/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2171 - mse: 1.2171 - mae: 0.8573 - val_loss: 2.9782 - val_mse: 2.9782 - val_mae: 1.2912\n",
      "Epoch 124/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.5579 - mse: 1.5579 - mae: 1.0062 - val_loss: 2.7116 - val_mse: 2.7116 - val_mae: 1.1946\n",
      "Epoch 125/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2656 - mse: 1.2656 - mae: 0.8723 - val_loss: 2.4548 - val_mse: 2.4548 - val_mae: 1.1065\n",
      "Epoch 126/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3041 - mse: 1.3041 - mae: 0.9122 - val_loss: 2.5914 - val_mse: 2.5914 - val_mae: 1.1316\n",
      "Epoch 127/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.6243 - mse: 1.6243 - mae: 1.0078 - val_loss: 3.1515 - val_mse: 3.1515 - val_mae: 1.3198\n",
      "Epoch 128/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4845 - mse: 1.4845 - mae: 0.9386 - val_loss: 2.5419 - val_mse: 2.5419 - val_mae: 1.1136\n",
      "Epoch 129/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4484 - mse: 1.4484 - mae: 0.9424 - val_loss: 2.3845 - val_mse: 2.3845 - val_mae: 1.0796\n",
      "Epoch 130/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.5135 - mse: 1.5135 - mae: 0.9723 - val_loss: 2.3803 - val_mse: 2.3803 - val_mae: 1.0933\n",
      "Epoch 131/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.4432 - mse: 1.4432 - mae: 0.9115 - val_loss: 2.8430 - val_mse: 2.8430 - val_mae: 1.2254\n",
      "Epoch 132/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.5545 - mse: 1.5545 - mae: 0.9797 - val_loss: 2.4516 - val_mse: 2.4516 - val_mae: 1.1030\n",
      "Epoch 133/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3519 - mse: 1.3519 - mae: 0.8973 - val_loss: 2.6146 - val_mse: 2.6146 - val_mae: 1.1708\n",
      "Epoch 134/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3858 - mse: 1.3858 - mae: 0.9375 - val_loss: 2.1656 - val_mse: 2.1656 - val_mae: 1.0527\n",
      "Epoch 135/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3252 - mse: 1.3252 - mae: 0.9109 - val_loss: 2.5412 - val_mse: 2.5412 - val_mae: 1.1495\n",
      "Epoch 136/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.5190 - mse: 1.5190 - mae: 0.9757 - val_loss: 2.6217 - val_mse: 2.6217 - val_mae: 1.1878\n",
      "Epoch 137/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.2936 - mse: 1.2936 - mae: 0.8834 - val_loss: 2.2720 - val_mse: 2.2720 - val_mae: 1.0645\n",
      "Epoch 138/300\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 1.5322 - mse: 1.5322 - mae: 0.9640 - val_loss: 2.3763 - val_mse: 2.3763 - val_mae: 1.1269\n",
      "Epoch 139/300\n",
      "10/10 [==============================] - 2s 200ms/step - loss: 1.2270 - mse: 1.2270 - mae: 0.8459 - val_loss: 2.7372 - val_mse: 2.7372 - val_mae: 1.2153\n",
      "Epoch 140/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2110 - mse: 1.2110 - mae: 0.8402 - val_loss: 2.3805 - val_mse: 2.3805 - val_mae: 1.0952\n",
      "Epoch 141/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3428 - mse: 1.3428 - mae: 0.8811 - val_loss: 2.7971 - val_mse: 2.7971 - val_mae: 1.2123\n",
      "Epoch 142/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4572 - mse: 1.4572 - mae: 0.9705 - val_loss: 2.2757 - val_mse: 2.2757 - val_mae: 1.0692\n",
      "Epoch 143/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3029 - mse: 1.3029 - mae: 0.8870 - val_loss: 2.3013 - val_mse: 2.3013 - val_mae: 1.0730\n",
      "Epoch 144/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.2416 - mse: 1.2416 - mae: 0.8563 - val_loss: 2.7657 - val_mse: 2.7657 - val_mae: 1.1980\n",
      "Epoch 145/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4245 - mse: 1.4245 - mae: 0.9350 - val_loss: 2.7310 - val_mse: 2.7310 - val_mae: 1.1789\n",
      "Epoch 146/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4679 - mse: 1.4679 - mae: 0.9574 - val_loss: 2.6704 - val_mse: 2.6704 - val_mae: 1.1677\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2859 - mse: 1.2859 - mae: 0.9074 - val_loss: 2.7300 - val_mse: 2.7300 - val_mae: 1.1909\n",
      "Epoch 148/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1100 - mse: 1.1100 - mae: 0.8292 - val_loss: 2.4715 - val_mse: 2.4715 - val_mae: 1.1080\n",
      "Epoch 149/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.5001 - mse: 1.5001 - mae: 0.9457 - val_loss: 2.2731 - val_mse: 2.2731 - val_mae: 1.0687\n",
      "Epoch 150/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2242 - mse: 1.2242 - mae: 0.8612 - val_loss: 2.5182 - val_mse: 2.5182 - val_mae: 1.1339\n",
      "Epoch 151/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3976 - mse: 1.3976 - mae: 0.9398 - val_loss: 2.4153 - val_mse: 2.4153 - val_mae: 1.1237\n",
      "Epoch 152/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4476 - mse: 1.4476 - mae: 0.9377 - val_loss: 2.2834 - val_mse: 2.2834 - val_mae: 1.0829\n",
      "Epoch 153/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.4409 - mse: 1.4409 - mae: 0.9166 - val_loss: 2.2479 - val_mse: 2.2479 - val_mae: 1.0900\n",
      "Epoch 154/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.4084 - mse: 1.4084 - mae: 0.9466 - val_loss: 2.5644 - val_mse: 2.5644 - val_mae: 1.1531\n",
      "Epoch 155/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2763 - mse: 1.2763 - mae: 0.8800 - val_loss: 2.7882 - val_mse: 2.7882 - val_mae: 1.2396\n",
      "Epoch 156/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3396 - mse: 1.3396 - mae: 0.9152 - val_loss: 2.5575 - val_mse: 2.5575 - val_mae: 1.1452\n",
      "Epoch 157/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2111 - mse: 1.2111 - mae: 0.8612 - val_loss: 2.7063 - val_mse: 2.7063 - val_mae: 1.1979\n",
      "Epoch 158/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0826 - mse: 1.0826 - mae: 0.8281 - val_loss: 2.5310 - val_mse: 2.5310 - val_mae: 1.1345\n",
      "Epoch 159/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.1445 - mse: 1.1445 - mae: 0.8492 - val_loss: 2.4103 - val_mse: 2.4103 - val_mae: 1.1137\n",
      "Epoch 160/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.1552 - mse: 1.1552 - mae: 0.8237 - val_loss: 2.4682 - val_mse: 2.4682 - val_mae: 1.1418\n",
      "Epoch 161/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3891 - mse: 1.3891 - mae: 0.9327 - val_loss: 2.8671 - val_mse: 2.8671 - val_mae: 1.2602\n",
      "Epoch 162/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.4539 - mse: 1.4539 - mae: 0.9505 - val_loss: 2.8494 - val_mse: 2.8494 - val_mae: 1.2428\n",
      "Epoch 163/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3507 - mse: 1.3507 - mae: 0.9088 - val_loss: 2.3345 - val_mse: 2.3345 - val_mae: 1.0997\n",
      "Epoch 164/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3017 - mse: 1.3017 - mae: 0.8939 - val_loss: 2.7666 - val_mse: 2.7666 - val_mae: 1.2355\n",
      "Epoch 165/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.2591 - mse: 1.2591 - mae: 0.8678 - val_loss: 2.6184 - val_mse: 2.6184 - val_mae: 1.1842\n",
      "Epoch 166/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0647 - mse: 1.0647 - mae: 0.7758 - val_loss: 2.5880 - val_mse: 2.5880 - val_mae: 1.1913\n",
      "Epoch 167/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3329 - mse: 1.3329 - mae: 0.9051 - val_loss: 2.6875 - val_mse: 2.6875 - val_mae: 1.2031\n",
      "Epoch 168/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3991 - mse: 1.3991 - mae: 0.9355 - val_loss: 2.4141 - val_mse: 2.4141 - val_mae: 1.1121\n",
      "Epoch 169/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3201 - mse: 1.3201 - mae: 0.8933 - val_loss: 2.3390 - val_mse: 2.3390 - val_mae: 1.1028\n",
      "Epoch 170/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2637 - mse: 1.2637 - mae: 0.8841 - val_loss: 2.5772 - val_mse: 2.5772 - val_mae: 1.1769\n",
      "Epoch 171/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1134 - mse: 1.1134 - mae: 0.8182 - val_loss: 2.2478 - val_mse: 2.2478 - val_mae: 1.0745\n",
      "Epoch 172/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.0953 - mse: 1.0953 - mae: 0.8229 - val_loss: 2.2499 - val_mse: 2.2499 - val_mae: 1.0736\n",
      "Epoch 173/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2439 - mse: 1.2439 - mae: 0.9001 - val_loss: 2.2028 - val_mse: 2.2028 - val_mae: 1.0673\n",
      "Epoch 174/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2287 - mse: 1.2287 - mae: 0.8812 - val_loss: 2.3069 - val_mse: 2.3069 - val_mae: 1.0956\n",
      "Epoch 175/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2365 - mse: 1.2365 - mae: 0.8903 - val_loss: 2.4063 - val_mse: 2.4063 - val_mae: 1.1123\n",
      "Epoch 176/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0118 - mse: 1.0118 - mae: 0.7776 - val_loss: 2.3565 - val_mse: 2.3565 - val_mae: 1.1152\n",
      "Epoch 177/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2671 - mse: 1.2671 - mae: 0.8475 - val_loss: 2.5131 - val_mse: 2.5131 - val_mae: 1.1489\n",
      "Epoch 178/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.4278 - mse: 1.4278 - mae: 0.9429 - val_loss: 2.5786 - val_mse: 2.5786 - val_mae: 1.1564\n",
      "Epoch 179/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1652 - mse: 1.1652 - mae: 0.8534 - val_loss: 2.2450 - val_mse: 2.2450 - val_mae: 1.0605\n",
      "Epoch 180/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3209 - mse: 1.3209 - mae: 0.9090 - val_loss: 2.2518 - val_mse: 2.2518 - val_mae: 1.0706\n",
      "Epoch 181/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1780 - mse: 1.1780 - mae: 0.8385 - val_loss: 2.2393 - val_mse: 2.2393 - val_mae: 1.0877\n",
      "Epoch 182/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2473 - mse: 1.2473 - mae: 0.8761 - val_loss: 2.4411 - val_mse: 2.4411 - val_mae: 1.1372\n",
      "Epoch 183/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2118 - mse: 1.2118 - mae: 0.8338 - val_loss: 2.5334 - val_mse: 2.5334 - val_mae: 1.1688\n",
      "Epoch 184/300\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 1.2243 - mse: 1.2243 - mae: 0.8836 - val_loss: 2.8969 - val_mse: 2.8969 - val_mae: 1.2486\n",
      "Epoch 185/300\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 1.2897 - mse: 1.2897 - mae: 0.8723 - val_loss: 2.6339 - val_mse: 2.6339 - val_mae: 1.1736\n",
      "Epoch 186/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3294 - mse: 1.3294 - mae: 0.9078 - val_loss: 2.2618 - val_mse: 2.2618 - val_mae: 1.0904\n",
      "Epoch 187/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2072 - mse: 1.2072 - mae: 0.8651 - val_loss: 2.6983 - val_mse: 2.6983 - val_mae: 1.2252\n",
      "Epoch 188/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3232 - mse: 1.3232 - mae: 0.8618 - val_loss: 2.1964 - val_mse: 2.1964 - val_mae: 1.0677\n",
      "Epoch 189/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3203 - mse: 1.3203 - mae: 0.9064 - val_loss: 2.9896 - val_mse: 2.9896 - val_mae: 1.2966\n",
      "Epoch 190/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2287 - mse: 1.2287 - mae: 0.8371 - val_loss: 2.4785 - val_mse: 2.4785 - val_mae: 1.1506\n",
      "Epoch 191/300\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 1.1759 - mse: 1.1759 - mae: 0.8420 - val_loss: 2.6185 - val_mse: 2.6185 - val_mae: 1.1937\n",
      "Epoch 192/300\n",
      "10/10 [==============================] - 2s 201ms/step - loss: 1.3931 - mse: 1.3931 - mae: 0.9251 - val_loss: 2.4076 - val_mse: 2.4076 - val_mae: 1.1448\n",
      "Epoch 193/300\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 1.0490 - mse: 1.0490 - mae: 0.8009 - val_loss: 2.4087 - val_mse: 2.4087 - val_mae: 1.1402\n",
      "Epoch 194/300\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 1.2715 - mse: 1.2715 - mae: 0.8876 - val_loss: 2.1966 - val_mse: 2.1966 - val_mae: 1.0737\n",
      "Epoch 195/300\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 1.2250 - mse: 1.2250 - mae: 0.8732 - val_loss: 2.2702 - val_mse: 2.2702 - val_mae: 1.0886\n",
      "Epoch 196/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0104 - mse: 1.0104 - mae: 0.7955 - val_loss: 2.6311 - val_mse: 2.6311 - val_mae: 1.2055\n",
      "Epoch 197/300\n",
      "10/10 [==============================] - 2s 203ms/step - loss: 1.1686 - mse: 1.1686 - mae: 0.8336 - val_loss: 2.3318 - val_mse: 2.3318 - val_mae: 1.1106\n",
      "Epoch 198/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 1.3945 - mse: 1.3945 - mae: 0.9241 - val_loss: 2.3117 - val_mse: 2.3117 - val_mae: 1.1202\n",
      "Epoch 199/300\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 1.0512 - mse: 1.0512 - mae: 0.8167 - val_loss: 2.3413 - val_mse: 2.3413 - val_mae: 1.1374\n",
      "Epoch 200/300\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 1.3189 - mse: 1.3189 - mae: 0.9111 - val_loss: 2.2801 - val_mse: 2.2801 - val_mae: 1.1039\n",
      "Epoch 201/300\n",
      "10/10 [==============================] - 2s 200ms/step - loss: 1.0916 - mse: 1.0916 - mae: 0.8308 - val_loss: 2.3447 - val_mse: 2.3447 - val_mae: 1.1271\n",
      "Epoch 202/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1360 - mse: 1.1360 - mae: 0.8495 - val_loss: 2.2362 - val_mse: 2.2362 - val_mae: 1.0821\n",
      "Epoch 203/300\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 1.0478 - mse: 1.0478 - mae: 0.8114 - val_loss: 2.4807 - val_mse: 2.4807 - val_mae: 1.1399\n",
      "Epoch 204/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3019 - mse: 1.3019 - mae: 0.8805 - val_loss: 2.3258 - val_mse: 2.3258 - val_mae: 1.0992\n",
      "Epoch 205/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.0522 - mse: 1.0522 - mae: 0.8125 - val_loss: 2.2406 - val_mse: 2.2406 - val_mae: 1.0885\n",
      "Epoch 206/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9766 - mse: 0.9766 - mae: 0.7605 - val_loss: 2.1489 - val_mse: 2.1489 - val_mae: 1.0785\n",
      "Epoch 207/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2050 - mse: 1.2050 - mae: 0.8638 - val_loss: 2.3552 - val_mse: 2.3552 - val_mae: 1.1075\n",
      "Epoch 208/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1629 - mse: 1.1629 - mae: 0.8238 - val_loss: 3.2173 - val_mse: 3.2173 - val_mae: 1.3629\n",
      "Epoch 209/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1315 - mse: 1.1315 - mae: 0.7969 - val_loss: 2.6651 - val_mse: 2.6651 - val_mae: 1.2015\n",
      "Epoch 210/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1422 - mse: 1.1422 - mae: 0.8591 - val_loss: 2.4996 - val_mse: 2.4996 - val_mae: 1.1605\n",
      "Epoch 211/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.2691 - mse: 1.2691 - mae: 0.8761 - val_loss: 2.1366 - val_mse: 2.1366 - val_mae: 1.0600\n",
      "Epoch 212/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.2760 - mse: 1.2760 - mae: 0.8884 - val_loss: 2.2475 - val_mse: 2.2475 - val_mae: 1.0561\n",
      "Epoch 213/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.0064 - mse: 1.0064 - mae: 0.7827 - val_loss: 2.3007 - val_mse: 2.3007 - val_mae: 1.0657\n",
      "Epoch 214/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1895 - mse: 1.1895 - mae: 0.8553 - val_loss: 2.7014 - val_mse: 2.7014 - val_mae: 1.1752\n",
      "Epoch 215/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0714 - mse: 1.0714 - mae: 0.8303 - val_loss: 2.5387 - val_mse: 2.5387 - val_mae: 1.1547\n",
      "Epoch 216/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0448 - mse: 1.0448 - mae: 0.8164 - val_loss: 2.4502 - val_mse: 2.4502 - val_mae: 1.1352\n",
      "Epoch 217/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1487 - mse: 1.1487 - mae: 0.8324 - val_loss: 2.7439 - val_mse: 2.7439 - val_mae: 1.2323\n",
      "Epoch 218/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9422 - mse: 0.9422 - mae: 0.7532 - val_loss: 2.9711 - val_mse: 2.9711 - val_mae: 1.2992\n",
      "Epoch 219/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0673 - mse: 1.0673 - mae: 0.7981 - val_loss: 2.7447 - val_mse: 2.7447 - val_mae: 1.2299\n",
      "Epoch 220/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1579 - mse: 1.1579 - mae: 0.8248 - val_loss: 3.4678 - val_mse: 3.4678 - val_mae: 1.4408\n",
      "Epoch 221/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.3696 - mse: 1.3696 - mae: 0.9446 - val_loss: 2.6890 - val_mse: 2.6890 - val_mae: 1.2187\n",
      "Epoch 222/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0921 - mse: 1.0921 - mae: 0.8302 - val_loss: 2.4292 - val_mse: 2.4292 - val_mae: 1.1228\n",
      "Epoch 223/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.9309 - mse: 0.9309 - mae: 0.7565 - val_loss: 2.2559 - val_mse: 2.2559 - val_mae: 1.0802\n",
      "Epoch 224/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.9473 - mse: 0.9473 - mae: 0.7854 - val_loss: 2.5384 - val_mse: 2.5384 - val_mae: 1.1521\n",
      "Epoch 225/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1299 - mse: 1.1299 - mae: 0.8378 - val_loss: 2.3411 - val_mse: 2.3411 - val_mae: 1.1211\n",
      "Epoch 226/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3295 - mse: 1.3295 - mae: 0.9049 - val_loss: 2.7961 - val_mse: 2.7961 - val_mae: 1.2471\n",
      "Epoch 227/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.4378 - mse: 1.4378 - mae: 0.9162 - val_loss: 2.9139 - val_mse: 2.9139 - val_mae: 1.2974\n",
      "Epoch 228/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1625 - mse: 1.1625 - mae: 0.8750 - val_loss: 2.8982 - val_mse: 2.8982 - val_mae: 1.2814\n",
      "Epoch 229/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1605 - mse: 1.1605 - mae: 0.8450 - val_loss: 3.1731 - val_mse: 3.1731 - val_mae: 1.3589\n",
      "Epoch 230/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.1718 - mse: 1.1718 - mae: 0.8617 - val_loss: 2.9898 - val_mse: 2.9898 - val_mae: 1.2970\n",
      "Epoch 231/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1865 - mse: 1.1865 - mae: 0.8409 - val_loss: 2.5577 - val_mse: 2.5577 - val_mae: 1.1737\n",
      "Epoch 232/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.9503 - mse: 0.9503 - mae: 0.7955 - val_loss: 2.4403 - val_mse: 2.4403 - val_mae: 1.1227\n",
      "Epoch 233/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0614 - mse: 1.0614 - mae: 0.8069 - val_loss: 2.1705 - val_mse: 2.1705 - val_mae: 1.0600\n",
      "Epoch 234/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.9877 - mse: 0.9877 - mae: 0.7933 - val_loss: 2.2691 - val_mse: 2.2691 - val_mae: 1.0958\n",
      "Epoch 235/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9779 - mse: 0.9779 - mae: 0.7693 - val_loss: 2.1831 - val_mse: 2.1831 - val_mae: 1.0871\n",
      "Epoch 236/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.9663 - mse: 0.9663 - mae: 0.7828 - val_loss: 2.4434 - val_mse: 2.4434 - val_mae: 1.1331\n",
      "Epoch 237/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0680 - mse: 1.0680 - mae: 0.8412 - val_loss: 2.4192 - val_mse: 2.4192 - val_mae: 1.1274\n",
      "Epoch 238/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9461 - mse: 0.9461 - mae: 0.7708 - val_loss: 2.2581 - val_mse: 2.2581 - val_mae: 1.1043\n",
      "Epoch 239/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9510 - mse: 0.9510 - mae: 0.7761 - val_loss: 2.1957 - val_mse: 2.1957 - val_mae: 1.0715\n",
      "Epoch 240/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.0644 - mse: 1.0644 - mae: 0.8251 - val_loss: 2.4470 - val_mse: 2.4470 - val_mae: 1.1181\n",
      "Epoch 241/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9408 - mse: 0.9408 - mae: 0.7456 - val_loss: 2.3273 - val_mse: 2.3273 - val_mae: 1.1111\n",
      "Epoch 242/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1578 - mse: 1.1578 - mae: 0.8218 - val_loss: 3.1945 - val_mse: 3.1945 - val_mae: 1.3534\n",
      "Epoch 243/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.4179 - mse: 1.4179 - mae: 0.9493 - val_loss: 3.0211 - val_mse: 3.0211 - val_mae: 1.3123\n",
      "Epoch 244/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.2312 - mse: 1.2312 - mae: 0.8867 - val_loss: 2.6759 - val_mse: 2.6759 - val_mae: 1.1860\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1701 - mse: 1.1701 - mae: 0.8603 - val_loss: 2.7618 - val_mse: 2.7618 - val_mae: 1.2231\n",
      "Epoch 246/300\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 1.0713 - mse: 1.0713 - mae: 0.8141 - val_loss: 2.4335 - val_mse: 2.4335 - val_mae: 1.1280\n",
      "Epoch 247/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9673 - mse: 0.9673 - mae: 0.7952 - val_loss: 2.1536 - val_mse: 2.1536 - val_mae: 1.0667\n",
      "Epoch 248/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9989 - mse: 0.9989 - mae: 0.7765 - val_loss: 2.1633 - val_mse: 2.1633 - val_mae: 1.0728\n",
      "Epoch 249/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0319 - mse: 1.0319 - mae: 0.7958 - val_loss: 2.3440 - val_mse: 2.3440 - val_mae: 1.1107\n",
      "Epoch 250/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1462 - mse: 1.1462 - mae: 0.8359 - val_loss: 2.2303 - val_mse: 2.2303 - val_mae: 1.0948\n",
      "Epoch 251/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.0960 - mse: 1.0960 - mae: 0.8028 - val_loss: 2.5673 - val_mse: 2.5673 - val_mae: 1.1923\n",
      "Epoch 252/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.8956 - mse: 0.8956 - mae: 0.7342 - val_loss: 2.2796 - val_mse: 2.2796 - val_mae: 1.0991\n",
      "Epoch 253/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0382 - mse: 1.0382 - mae: 0.8007 - val_loss: 2.3726 - val_mse: 2.3726 - val_mae: 1.1103\n",
      "Epoch 254/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.1239 - mse: 1.1239 - mae: 0.8241 - val_loss: 2.1802 - val_mse: 2.1802 - val_mae: 1.0928\n",
      "Epoch 255/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.9776 - mse: 0.9776 - mae: 0.7920 - val_loss: 2.1235 - val_mse: 2.1235 - val_mae: 1.0904\n",
      "Epoch 256/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2917 - mse: 1.2917 - mae: 0.8723 - val_loss: 2.2000 - val_mse: 2.2000 - val_mae: 1.0796\n",
      "Epoch 257/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0506 - mse: 1.0506 - mae: 0.7857 - val_loss: 2.4930 - val_mse: 2.4930 - val_mae: 1.1381\n",
      "Epoch 258/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.0977 - mse: 1.0977 - mae: 0.8447 - val_loss: 2.6756 - val_mse: 2.6756 - val_mae: 1.2251\n",
      "Epoch 259/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.0705 - mse: 1.0705 - mae: 0.8181 - val_loss: 2.4449 - val_mse: 2.4449 - val_mae: 1.1535\n",
      "Epoch 260/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.3111 - mse: 1.3111 - mae: 0.9091 - val_loss: 2.8668 - val_mse: 2.8668 - val_mae: 1.2798\n",
      "Epoch 261/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 1.0374 - mse: 1.0374 - mae: 0.8162 - val_loss: 2.5601 - val_mse: 2.5601 - val_mae: 1.1990\n",
      "Epoch 262/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9292 - mse: 0.9292 - mae: 0.7481 - val_loss: 2.1584 - val_mse: 2.1584 - val_mae: 1.0692\n",
      "Epoch 263/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9045 - mse: 0.9045 - mae: 0.7384 - val_loss: 2.4963 - val_mse: 2.4963 - val_mae: 1.1715\n",
      "Epoch 264/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0785 - mse: 1.0785 - mae: 0.8376 - val_loss: 3.3469 - val_mse: 3.3469 - val_mae: 1.4033\n",
      "Epoch 265/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.2026 - mse: 1.2026 - mae: 0.8446 - val_loss: 2.9074 - val_mse: 2.9074 - val_mae: 1.2880\n",
      "Epoch 266/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0871 - mse: 1.0871 - mae: 0.8209 - val_loss: 2.2548 - val_mse: 2.2548 - val_mae: 1.1085\n",
      "Epoch 267/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.7840 - mse: 0.7840 - mae: 0.6974 - val_loss: 2.2919 - val_mse: 2.2919 - val_mae: 1.1074\n",
      "Epoch 268/300\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.9314 - mse: 0.9314 - mae: 0.7537 - val_loss: 2.2173 - val_mse: 2.2173 - val_mae: 1.1051\n",
      "Epoch 269/300\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 1.0751 - mse: 1.0751 - mae: 0.8225 - val_loss: 2.0793 - val_mse: 2.0793 - val_mae: 1.0644\n",
      "Epoch 270/300\n",
      " 4/10 [===========>..................] - ETA: 0s - loss: 1.0083 - mse: 1.0083 - mae: 0.7608"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "history=model.fit(X_train, y_train, epochs=300, batch_size=32, verbose=1, validation_split=0.4)\n",
    "predictions = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(predictions)\n",
    "plt.plot(y_val)\n",
    "plt.title('Predictions')\n",
    "plt.legend(['Predicted P values', 'Actual P Values'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79852620",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e238fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff4b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24aa009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdea305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576a362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5f0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e5b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a94d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4f535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0327a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828bf26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078a585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
